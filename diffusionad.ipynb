{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14156306,"sourceType":"datasetVersion","datasetId":9022902}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"11292d40-541d-4f23-9946-1d46cd35c85d","cell_type":"code","source":"!rm -rf diffusionAD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:18:25.521393Z","iopub.execute_input":"2025-12-14T18:18:25.521992Z","iopub.status.idle":"2025-12-14T18:18:25.646488Z","shell.execute_reply.started":"2025-12-14T18:18:25.521954Z","shell.execute_reply":"2025-12-14T18:18:25.645749Z"}},"outputs":[],"execution_count":null},{"id":"9fcf9b99-af80-4052-a1ed-1e9d4e59c671","cell_type":"code","source":"!git clone https://github.com/nguyenduchuyiu/diffusionAD.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:18:27.136793Z","iopub.execute_input":"2025-12-14T18:18:27.137064Z","iopub.status.idle":"2025-12-14T18:18:28.030479Z","shell.execute_reply.started":"2025-12-14T18:18:27.137038Z","shell.execute_reply":"2025-12-14T18:18:28.029807Z"}},"outputs":[],"execution_count":null},{"id":"e3df76b4-1322-4f59-801c-5d8bef7e4e48","cell_type":"code","source":"%cd diffusionAD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:18:30.324438Z","iopub.execute_input":"2025-12-14T18:18:30.324743Z","iopub.status.idle":"2025-12-14T18:18:30.335910Z","shell.execute_reply.started":"2025-12-14T18:18:30.324714Z","shell.execute_reply":"2025-12-14T18:18:30.335143Z"}},"outputs":[],"execution_count":null},{"id":"7dc6ccda-4df0-46a8-b05d-b29cfd5ba7e4","cell_type":"code","source":"%%writefile args/args2.json\n{\n  \"img_size\": [256,256],\n  \"Batch_Size\": 8,\n  \"EPOCHS\": 800,\n  \"T\": 1000,\n  \"base_channels\": 128,\n  \"beta_schedule\": \"linear\",\n  \"loss_type\": \"l2\",\n  \"diffusion_lr\": 1e-4,\n  \"seg_lr\": 1e-5,\n  \"random_slice\": true,\n  \"weight_decay\": 0.0,\n  \"save_imgs\":true,\n  \"save_vids\":false, \n  \"dropout\":0,\n  \"attention_resolutions\":\"32,16,8\",\n  \"num_heads\":4,\n  \"num_head_channels\":-1,\n  \"noise_fn\":\"gauss\",\n  \"channels\":1,\n  \"data_name\":\"denso_dataset\",\n  \"data_root_path\":\"/kaggle/input/denso-dataset\",\n  \"anomaly_source_path\":\"/kaggle/input/denso-dataset/dtd\",\n  \"noisier_t_range\":600,\n  \"less_t_range\":300,\n  \"condition_w\":1,\n  \"eval_normal_t\":200,\n  \"eval_noisier_t\":400,\n  \"output_path\":\"outputs\",\n  \"gradient_accumulation_steps\": 2,\n  \"use_mixed_precision\": false,\n  \"channel_mults\": [1, 1, 2, 2, 4, 4],\n  \"loss_weight\": \"uniform\",\n  \"loss-type\": \"l2\",\n  \"resume_training\": true,\n  \"use_gradient_checkpointing\": true,\n  \"use_bfloat16\": false\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:31:07.775000Z","iopub.execute_input":"2025-12-14T18:31:07.775515Z","iopub.status.idle":"2025-12-14T18:31:07.781059Z","shell.execute_reply.started":"2025-12-14T18:31:07.775490Z","shell.execute_reply":"2025-12-14T18:31:07.780244Z"}},"outputs":[],"execution_count":null},{"id":"f29c1ded-63c8-4a70-9804-5c906bf64d03","cell_type":"code","source":"!python3 -u src/train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:31:09.472850Z","iopub.execute_input":"2025-12-14T18:31:09.473420Z","iopub.status.idle":"2025-12-14T18:36:43.609476Z","shell.execute_reply.started":"2025-12-14T18:31:09.473397Z","shell.execute_reply":"2025-12-14T18:36:43.608724Z"}},"outputs":[],"execution_count":null},{"id":"1b84faf3-effe-42d3-afe7-00193e4bed7b","cell_type":"code","source":"%%writefile src/inference.py\nimport matplotlib.pyplot as plt\nimport torch\nfrom skimage.metrics import structural_similarity as ssim\nfrom sklearn.metrics import auc, roc_curve,average_precision_score\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\nimport cv2\nimport torch.nn as nn\nfrom models import UNetModel, update_ema_params\nfrom models import SegmentationSubNetwork\nimport torch.nn as nn\nfrom utils import RealIADTestDataset\nfrom models import GaussianDiffusionModel, get_beta_schedule\nfrom math import exp\nimport torch.nn.functional as F\ntorch.cuda.empty_cache()\nfrom tqdm import tqdm\nimport json\nimport os\nfrom collections import defaultdict\nimport pandas as pd\nimport torchvision.utils\nimport os\nfrom torch.utils.data import DataLoader\nfrom skimage.measure import label, regionprops\nimport sys\nfrom utils import BinaryFocalLoss\n\ndef preprocess_image(image_path, img_size=(256, 256), channels=3):\n    image = cv2.imread(image_path)\n    if channels == 1:\n        # Convert to grayscale\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.resize(image, (img_size[1], img_size[0]))\n        image = (image / 255.0).astype(np.float32)\n        image = torch.from_numpy(image).unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (img_size[1], img_size[0]))\n        image = (image / 255.0)\n        image = np.transpose(image.astype(np.float32), (2, 0, 1))\n        image = torch.from_numpy(image).unsqueeze(0)  # (1, 3, H, W)\n    return image\n\ndef denormalize_image(tensor_image):\n    img = tensor_image.cpu().squeeze(0)\n    if img.dim() == 2:  # Grayscale (H, W)\n        img_np = img.numpy()\n    elif img.shape[0] == 1:  # Grayscale (1, H, W)\n        img_np = img.squeeze(0).numpy()\n    else:  # RGB (C, H, W)\n        img_np = img.permute(1, 2, 0).numpy()\n    img_np = (img_np + 1) / 2.0\n    img_np = np.nan_to_num(img_np, nan=0.0, posinf=1.0, neginf=0.0)\n    img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n    # Convert grayscale to RGB for display\n    if len(img_np.shape) == 2:\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    return img_np\n\ndef gridify_output(img, row_size=-1):\n    scale_img = lambda img: ((img + 1) * 127.5).clamp(0, 255).to(torch.uint8)\n    return torchvision.utils.make_grid(scale_img(img), nrow=row_size, pad_value=-1).cpu().data.permute(\n            0, 2,\n            1\n            ).contiguous().permute(\n            2, 1, 0\n            )\n\ndef defaultdict_from_json(jsonDict):\n    func = lambda: defaultdict(str)\n    dd = func()\n    dd.update(jsonDict)\n    return dd\n\ndef load_checkpoint(ckpt_path, device):\n\n    print(\"checkpoint\",ckpt_path)\n\n    from collections import defaultdict\n    try:\n        torch.serialization.add_safe_globals([defaultdict])\n    except Exception:\n        pass\n\n    loaded_model = torch.load(ckpt_path, map_location=device, weights_only=False)\n    return loaded_model\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\ndef image_transform(image):\n     return np.clip(image* 255, 0, 255).astype(np.uint8)\n \ndef cvt2heatmap(gray):\n    heatmap = cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET)\n    return heatmap\ndef show_cam_on_image(img, anomaly_map):\n    cam = np.float32(anomaly_map)/255 + np.float32(img)/255\n    max_val = np.max(cam)\n    if max_val > 0:\n        cam = cam / max_val\n    return np.uint8(255 * cam) \n\n\ndef min_max_norm(image):\n    a_min, a_max = image.min(), image.max()\n    if a_max - a_min == 0:\n        return np.zeros_like(image)\n    return (image-a_min)/(a_max - a_min)\n\n        \ndef predict(unet_model, seg_model, ddpm, image_tensor, args, device='cpu', heatmap_threshold=0.6):\n    \"\"\"\n    Hàm predict giờ nhận vào image_tensor đã được xử lý và args từ model.\n    \"\"\"\n    normal_t = args[\"eval_normal_t\"]\n    noiser_t = args[\"eval_noisier_t\"]\n    \n    image_tensor = image_tensor.to(device)\n\n    normal_t_tensor = torch.tensor([normal_t], device=device).repeat(image_tensor.shape[0])\n    noiser_t_tensor = torch.tensor([noiser_t], device=device).repeat(image_tensor.shape[0])\n\n    with torch.no_grad():\n        _, pred_x_0_condition, pred_x_0_normal, pred_x_0_noisier, x_normal_t, x_noiser_t, pred_x_t_noisier = ddpm.norm_guided_one_step_denoising_eval(unet_model, image_tensor, normal_t_tensor, noiser_t_tensor, args)\n        pred_mask_logits = seg_model(torch.cat((image_tensor, pred_x_0_condition), dim=1))\n            \n    pred_mask = torch.sigmoid(pred_mask_logits)\n    out_mask = pred_mask\n\n    # Tính điểm anomaly\n    topk_out_mask = torch.flatten(out_mask[0], start_dim=1)\n    topk_out_mask = torch.topk(topk_out_mask, 50, dim=1, largest=True)[0]\n    image_score = torch.mean(topk_out_mask)\n\n    # --- Visualisation ---\n    # Original image: convert from [0, 1] directly to [0, 255] (not [-1, 1])\n    img = image_tensor.cpu().squeeze(0)\n    if img.shape[0] == 1:  # Grayscale (1, H, W)\n        raw_image_np = img.squeeze(0).numpy()\n        raw_image = np.clip(raw_image_np * 255.0, 0, 255).astype(np.uint8)\n        raw_image = cv2.cvtColor(raw_image, cv2.COLOR_GRAY2RGB)\n    else:  # RGB (C, H, W)\n        raw_image_np = img.permute(1, 2, 0).numpy()\n        raw_image = np.clip(raw_image_np * 255.0, 0, 255).astype(np.uint8)\n    \n    # Reconstructed images: use denormalize (model outputs are in [-1, 1])\n    recon_condition = denormalize_image(pred_x_0_condition)\n    recon_normal_t = denormalize_image(pred_x_0_normal)\n    recon_noisier_t = denormalize_image(pred_x_0_noisier)\n    \n    # Create heatmap with higher contrast\n    mask_data = out_mask[0, 0].cpu().numpy().astype(np.float32)\n    mask_data[mask_data < heatmap_threshold] = 0\n    \n    # Apply contrast enhancement using gamma correction\n    gamma = 0.1  # Lower gamma = higher contrast for bright areas\n    mask_data_enhanced = np.power(np.clip(mask_data, 0, 1), gamma)\n    \n    ano_map = cv2.GaussianBlur(mask_data_enhanced, (15, 15), 4)\n    ano_map = min_max_norm(ano_map)\n    ano_map = np.nan_to_num(ano_map, nan=0.0)\n    \n    # Use HOT colormap for better visibility (red/yellow/white)\n    ano_map_heatmap = cv2.applyColorMap(np.uint8(np.clip(ano_map * 255.0, 0, 255)), cv2.COLORMAP_HOT)\n    \n    # Create overlay\n    raw_image_bgr = cv2.cvtColor(raw_image, cv2.COLOR_RGB2BGR)\n    ano_map_overlay = show_cam_on_image(raw_image_bgr, ano_map_heatmap)\n    ano_map_overlay = cv2.cvtColor(ano_map_overlay, cv2.COLOR_BGR2RGB)\n    \n    # Hiển thị\n    f, axes = plt.subplots(1, 5, figsize=(20, 4))\n    f.suptitle(f'Anomaly Score: {image_score:.4f}')\n\n    axes[0].imshow(raw_image)\n    axes[0].set_title('Input')\n    axes[0].axis('off')\n\n    axes[1].imshow(recon_condition)\n    axes[1].set_title('Reconstruction')\n    axes[1].axis('off')\n    \n    axes[2].imshow(recon_noisier_t)\n    axes[2].set_title('Recon (Noisier)')\n    axes[2].axis('off')\n    \n    # Create enhanced anomaly mask with high contrast\n    mask_raw = out_mask[0][0].cpu().numpy().astype(np.float32)\n    mask_raw[mask_raw < heatmap_threshold] = 0\n    mask_enhanced = np.power(np.clip(mask_raw, 0, 1), 0.3)\n    mask_normalized = min_max_norm(mask_enhanced)\n    mask_normalized = np.nan_to_num(mask_normalized, nan=0.0)\n    mask_stretched = np.uint8(np.clip(mask_normalized * 255.0, 0, 255))\n    anomaly_mask_colored = cv2.applyColorMap(mask_stretched, cv2.COLORMAP_HOT)\n    anomaly_mask_colored = cv2.cvtColor(anomaly_mask_colored, cv2.COLOR_BGR2RGB)\n    \n    axes[3].imshow(anomaly_mask_colored)\n    axes[3].set_title('Anomaly Mask')\n    axes[3].axis('off')\n\n    axes[4].imshow(ano_map_overlay)\n    axes[4].set_title('Heatmap Overlay')\n    axes[4].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(\"out.png\")\n    plt.show()\n\ndef predict_image(unet_model, seg_model, ddpm, image_path, args, device='cpu', heatmap_threshold=0.6):\n    image_tensor = preprocess_image(image_path, img_size=args['img_size'], channels=args['channels'])\n    image_tensor = image_tensor.to(device)\n    return predict(unet_model, seg_model, ddpm, image_tensor, args, device, heatmap_threshold)\n\ndef predict_batch(unet_model, seg_model, ddpm, image_arrays, args, device='cpu', heatmap_threshold=0.6, batch_size=8, progress_callback=None):\n    \"\"\"\n    Predict anomalies for a batch of images with parallel processing\n    Args:\n        unet_model: UNet model\n        seg_model: Segmentation model\n        ddpm: DDPM model\n        image_arrays: List of numpy image arrays\n        args: Model arguments\n        device: Device to run on\n        heatmap_threshold: Threshold for anomaly detection\n        batch_size: Batch size for parallel processing\n        progress_callback: Optional callback function(progress, status_text) for progress updates\n    Returns:\n        List of prediction results with inference_time\n    \"\"\"\n    results = []\n    total_images = len(image_arrays)\n    num_batches = (total_images + batch_size - 1) // batch_size\n    \n    # Process images in batches\n    for batch_idx in range(0, len(image_arrays), batch_size):\n        batch_images = image_arrays[batch_idx:batch_idx + batch_size]\n        current_batch = batch_idx // batch_size + 1\n        \n        # Update progress\n        if progress_callback is not None:\n            progress = batch_idx / total_images\n            status_text = f'Processing batch {current_batch}/{num_batches} ({len(batch_images)} images)...'\n            progress_callback(progress, status_text)\n        \n        # Preprocess batch - process all images in parallel\n        batch_tensors = []\n        for image_array in batch_images:\n            if len(image_array.shape) == 3:\n                # Check if model expects grayscale\n                if args.get('channels', 3) == 1:\n                    gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n                    image_tensor = torch.from_numpy(gray.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)\n                else:\n                    image_tensor = torch.from_numpy(np.transpose(image_array.astype(np.float32) / 255.0, (2, 0, 1))).unsqueeze(0)\n            elif len(image_array.shape) == 2:\n                image_tensor = torch.from_numpy(image_array.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)\n            else:\n                raise ValueError(\"Image must be 2D (H, W) or 3D array (H, W, C)\")\n            \n            # Resize to model input size\n            image_tensor = torch.nn.functional.interpolate(image_tensor, size=args['img_size'], mode='bilinear', align_corners=False)\n            batch_tensors.append(image_tensor)\n        \n        # Stack into batch tensor - this enables parallel processing on GPU\n        batch_tensor = torch.cat(batch_tensors, dim=0).to(device)\n        \n        normal_t = args[\"eval_normal_t\"]\n        noiser_t = args[\"eval_noisier_t\"]\n        \n        normal_t_tensor = torch.tensor([normal_t], device=device).repeat(batch_tensor.shape[0])\n        noiser_t_tensor = torch.tensor([noiser_t], device=device).repeat(batch_tensor.shape[0])\n        \n        # Synchronize GPU before timing (for accurate measurement)\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        \n        # Measure inference time for batch - all images processed in parallel\n        inference_start = time.time()\n        with torch.no_grad():\n            _, pred_x_0_condition, pred_x_0_normal, pred_x_0_noisier, x_normal_t, x_noiser_t, pred_x_t_noisier = ddpm.norm_guided_one_step_denoising_eval(unet_model, batch_tensor, normal_t_tensor, noiser_t_tensor, args)\n            pred_mask_logits = seg_model(torch.cat((batch_tensor, pred_x_0_condition), dim=1))\n        \n        # Synchronize GPU after inference (for accurate timing)\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        \n        batch_inference_time = time.time() - inference_start\n        \n        pred_mask = torch.sigmoid(pred_mask_logits)\n        out_mask = pred_mask\n        \n        # Process each image in batch (post-processing)\n        for i in range(batch_tensor.shape[0]):\n            # Calculate anomaly score\n            topk_out_mask = torch.flatten(out_mask[i], start_dim=1)\n            topk_out_mask = torch.topk(topk_out_mask, 50, dim=1, largest=True)[0]\n            image_score = torch.mean(topk_out_mask).cpu().item()\n            \n            # Each image in batch is processed in parallel, so effective latency per image\n            # is approximately the batch time (since they run simultaneously on GPU)\n            # We record the batch time as the inference time for each image\n            per_image_time = batch_inference_time\n            \n            results.append({\n                \"anomaly_score\": float(image_score),\n                \"is_anomaly\": image_score > heatmap_threshold,\n                \"inference_time\": per_image_time\n            })\n        \n        # Update progress after processing batch\n        if progress_callback is not None:\n            progress = min((batch_idx + len(batch_images)) / total_images, 1.0)\n            status_text = f'Completed batch {current_batch}/{num_batches} ({len(results)}/{total_images} images processed)...'\n            progress_callback(progress, status_text)\n    \n    return results\n\ndef predict_single_tensor(unet_model, seg_model, ddpm, image_tensor, args, device='cpu', heatmap_threshold=0.6, return_visualizations=True):\n    \"\"\"\n    Predict anomaly for a single image tensor\n    \"\"\"\n    normal_t = args[\"eval_normal_t\"]\n    noiser_t = args[\"eval_noisier_t\"]\n    \n    image_tensor = image_tensor.to(device)\n    normal_t_tensor = torch.tensor([normal_t], device=device).repeat(image_tensor.shape[0])\n    noiser_t_tensor = torch.tensor([noiser_t], device=device).repeat(image_tensor.shape[0])\n\n    with torch.no_grad():\n        _, pred_x_0_condition, pred_x_0_normal, pred_x_0_noisier, x_normal_t, x_noiser_t, pred_x_t_noisier = ddpm.norm_guided_one_step_denoising_eval(unet_model, image_tensor, normal_t_tensor, noiser_t_tensor, args)\n        pred_mask_logits = seg_model(torch.cat((image_tensor, pred_x_0_condition), dim=1))\n            \n    pred_mask = torch.sigmoid(pred_mask_logits)\n    out_mask = pred_mask\n\n    # Calculate anomaly score\n    topk_out_mask = torch.flatten(out_mask[0], start_dim=1)\n    topk_out_mask = torch.topk(topk_out_mask, 50, dim=1, largest=True)[0]\n    image_score = torch.mean(topk_out_mask).cpu().item()\n\n    result = {\n        \"anomaly_score\": float(image_score),\n        \"is_anomaly\": image_score > heatmap_threshold\n    }\n    \n    if return_visualizations:\n        # Create visualizations\n        # Original image: convert from [0, 1] directly to [0, 255] (not [-1, 1])\n        img = image_tensor.cpu().squeeze(0)\n        if img.shape[0] == 1:  # Grayscale (1, H, W)\n            raw_image_np = img.squeeze(0).numpy()\n            raw_image = np.clip(raw_image_np * 255.0, 0, 255).astype(np.uint8)\n            raw_image = cv2.cvtColor(raw_image, cv2.COLOR_GRAY2RGB)\n        else:  # RGB (C, H, W)\n            raw_image_np = img.permute(1, 2, 0).numpy()\n            raw_image = np.clip(raw_image_np * 255.0, 0, 255).astype(np.uint8)\n        \n        # Reconstructed images: use denormalize (model outputs are in [-1, 1])\n        recon_condition = denormalize_image(pred_x_0_condition)\n        recon_noisier_t = denormalize_image(pred_x_0_noisier)\n        \n        # Create heatmap with higher contrast\n        mask_data = out_mask[0, 0].cpu().numpy().astype(np.float32)\n        mask_data[mask_data < heatmap_threshold] = 0\n        \n        # Apply contrast enhancement using gamma correction\n        gamma = 0.1  # Lower gamma = higher contrast for bright areas\n        mask_data_enhanced = np.power(np.clip(mask_data, 0, 1), gamma)\n        \n        ano_map = cv2.GaussianBlur(mask_data_enhanced, (15, 15), 4)\n        ano_map = min_max_norm(ano_map)\n        ano_map = np.nan_to_num(ano_map, nan=0.0)\n        \n        # Use HOT colormap for better visibility (red/yellow/white)\n        ano_map_heatmap = cv2.applyColorMap(np.uint8(np.clip(ano_map * 255.0, 0, 255)), cv2.COLORMAP_HOT)\n        \n        # Create overlay\n        raw_image_bgr = cv2.cvtColor(raw_image, cv2.COLOR_RGB2BGR)\n        ano_map_overlay = show_cam_on_image(raw_image_bgr, ano_map_heatmap)\n        ano_map_overlay = cv2.cvtColor(ano_map_overlay, cv2.COLOR_BGR2RGB)\n        \n        # Create enhanced anomaly mask with high contrast\n        mask_raw = out_mask[0][0].cpu().numpy().astype(np.float32)\n        mask_raw[mask_raw < heatmap_threshold] = 0\n        mask_enhanced = np.power(np.clip(mask_raw, 0, 1), 0.3)\n        mask_normalized = min_max_norm(mask_enhanced)\n        mask_normalized = np.nan_to_num(mask_normalized, nan=0.0)\n        mask_stretched = np.uint8(np.clip(mask_normalized * 255.0, 0, 255))\n        anomaly_mask_colored = cv2.applyColorMap(mask_stretched, cv2.COLORMAP_HOT)\n        anomaly_mask_colored = cv2.cvtColor(anomaly_mask_colored, cv2.COLOR_BGR2RGB)\n        \n        result.update({\n            \"original_image\": raw_image,\n            \"reconstructed_image\": recon_condition,\n            \"recon_noisier\": recon_noisier_t,\n            \"anomaly_mask\": anomaly_mask_colored,\n            \"heatmap_overlay\": ano_map_overlay,\n            \"heatmap\": ano_map\n        })\n    \n    return result\n\ndef predict_single_image_array(unet_model, seg_model, ddpm, image_array, args, device='cpu', heatmap_threshold=0.6):\n    \"\"\"\n    Predict anomaly for a single image array (numpy array)\n    Args:\n        unet_model: UNet model\n        seg_model: Segmentation model\n        ddpm: DDPM model\n        image_array: numpy array of shape (H, W, C) with values in [0, 255]\n        args: Model arguments\n        device: Device to run on\n        heatmap_threshold: Threshold for anomaly detection\n    Returns:\n        Dictionary with prediction results and visualizations\n    \"\"\"\n    import time\n    \n    # Preprocess image\n    if len(image_array.shape) == 3:\n        # Check if model expects grayscale\n        if args.get('channels', 3) == 1:\n            # Convert RGB to grayscale\n            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n            image_tensor = torch.from_numpy(gray.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n        else:\n            image_tensor = torch.from_numpy(np.transpose(image_array.astype(np.float32) / 255.0, (2, 0, 1))).unsqueeze(0)\n    elif len(image_array.shape) == 2:\n        # Already grayscale\n        image_tensor = torch.from_numpy(image_array.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)\n    else:\n        raise ValueError(\"Image must be 2D (H, W) or 3D array (H, W, C)\")\n    \n    # Resize to model input size\n    image_tensor = torch.nn.functional.interpolate(image_tensor, size=args['img_size'], mode='bilinear', align_corners=False)\n    \n    # Measure inference time\n    inference_start = time.time()\n    result = predict_single_tensor(unet_model, seg_model, ddpm, image_tensor, args, device, heatmap_threshold, return_visualizations=True)\n    inference_time = time.time() - inference_start\n    \n    result['inference_time'] = inference_time\n    return result\n\nif __name__ == \"__main__\":\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # device = torch.device(\"cpu\")\n    print(f\"Using device: {device}\")\n    \n    ckpt_path = \"outputs/model/diff-params-ARGS=2/mat_tru/params-last.pt\"\n    image_path = \"/kaggle/input/denso-dataset/denso_dataset/mat_tru/test/bad/7_2024_10_6_10_0_2_1886_P4.png\"\n    # ckpt_path = \"params-best (1).pt\"\n    # image_path = \"datasets/denso_dataset/mat_tru/test/bad/7_2024_9_5_11_25_49_8_P4.png\"\n    heatmap_threshold = 0.5\n    # 1. Load checkpoint và lấy args từ đó\n    ckpt_state = load_checkpoint(ckpt_path, device)\n    args = ckpt_state['args']\n    args = defaultdict_from_json(args)\n    print(args)\n    \n    # 2. Khởi tạo model với args đã load\n    unet_model = UNetModel(args['img_size'][0], args['base_channels'], channel_mults=args['channel_mults'], dropout=args[\n                \"dropout\"], n_heads=args[\"num_heads\"], n_head_channels=args[\"num_head_channels\"],\n            in_channels=args[\"channels\"]\n            ).to(device)\n\n    seg_model = SegmentationSubNetwork(in_channels=args[\"channels\"] * 2, out_channels=1).to(device)\n\n    # 3. Khởi tạo DDPM\n    betas = get_beta_schedule(args['T'], args['beta_schedule'])\n    \n    ddpm =  GaussianDiffusionModel(\n            args['img_size'], betas, loss_weight=args['loss_weight'],\n            loss_type=args['loss-type'], noise=args[\"noise_fn\"], img_channels=args[\"channels\"]\n            )\n    \n    # 4. Load state dicts vào model\n    unet_model.load_state_dict(ckpt_state['unet_model_state_dict'])\n    seg_model.load_state_dict(ckpt_state['seg_model_state_dict'])\n    unet_model.eval()\n    seg_model.eval()\n\n\n    # 5. Chạy predict\n    predict_image(unet_model, seg_model, ddpm, image_path, args, device, heatmap_threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:36:43.611411Z","iopub.execute_input":"2025-12-14T18:36:43.611638Z","iopub.status.idle":"2025-12-14T18:36:43.626630Z","shell.execute_reply.started":"2025-12-14T18:36:43.611616Z","shell.execute_reply":"2025-12-14T18:36:43.625882Z"}},"outputs":[],"execution_count":null},{"id":"71ed6fe5-7133-4ff1-ba49-988199e2167a","cell_type":"code","source":"!python3 src/inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:36:43.627316Z","iopub.execute_input":"2025-12-14T18:36:43.627532Z","iopub.status.idle":"2025-12-14T18:36:53.145597Z","shell.execute_reply.started":"2025-12-14T18:36:43.627518Z","shell.execute_reply":"2025-12-14T18:36:53.144895Z"}},"outputs":[],"execution_count":null},{"id":"5a7e19ab-9c93-44d1-85af-ba66af604f20","cell_type":"code","source":"from IPython.display import Image, display\ndisplay(Image(\"out.png\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:38:08.793686Z","iopub.execute_input":"2025-12-14T18:38:08.793974Z","iopub.status.idle":"2025-12-14T18:38:08.842492Z","shell.execute_reply.started":"2025-12-14T18:38:08.793940Z","shell.execute_reply":"2025-12-14T18:38:08.841456Z"}},"outputs":[],"execution_count":null}]}