from random import seed
import torch
import os
import json
import matplotlib.pyplot as plt
import numpy as np
import signal
import sys
from torch import optim
from torch.utils.data import DataLoader
from models import UNetModel, update_ema_params
from models import SegmentationSubNetwork
from tqdm import tqdm
import torch.nn as nn
from utils import RealIADTrainDataset, RealIADTestDataset
from math import exp
import torch.nn.functional as F
from torch.amp import autocast
from torch.cuda.amp import GradScaler
from models import GaussianDiffusionModel, get_beta_schedule
from scipy.ndimage import gaussian_filter
from skimage.measure import label, regionprops
from sklearn.metrics import roc_auc_score,auc,average_precision_score
import pandas as pd
from collections import defaultdict
import time
import psutil
import gc
from utils import PerformanceProfiler

def weights_init(m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            m.weight.data.normal_(0.0, 0.02)
        elif classname.find('BatchNorm') != -1:
            m.weight.data.normal_(1.0, 0.02)
            m.bias.data.fill_(0)    

def defaultdict_from_json(jsonDict):
    func = lambda: defaultdict(str)
    dd = func()
    dd.update(jsonDict)
    return dd

class BinaryFocalLoss(nn.Module):
    def __init__(self, alpha=0.5, gamma=4, logits=False, reduce=True): 
        super(BinaryFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.reduce = reduce

    def forward(self, inputs, targets):
        # Always use binary_cross_entropy_with_logits for mixed precision compatibility
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss

def count_parameters(model):
    """Count the number of trainable parameters in a model"""
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params

def print_model_info(model, model_name):
    """Print detailed information about model parameters"""
    total_params = count_parameters(model)
    print(f"{model_name}: {total_params:,} trainable parameters")
    
    # Memory estimation (FP32)
    param_memory_mb = total_params * 4 / (1024 ** 2)
    print(f"{model_name} Memory: ~{param_memory_mb:.1f} MB")
    
    return total_params

def plot_learning_curves(train_loss_list, train_noise_loss_list, train_focal_loss_list, 
                        train_smL1_loss_list, loss_x_list, image_auroc_list, 
                        pixel_auroc_list, performance_x_list, sub_class, args, inline=False):
    """Plot simple loss curves only - minimal memory usage"""
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # Plot losses only
    if len(loss_x_list) > 0:
        ax.plot(loss_x_list, train_loss_list, 'b-', label='Total Loss', linewidth=2)
        ax.plot(loss_x_list, train_noise_loss_list, 'r-', label='Noise Loss')
        ax.plot(loss_x_list, train_focal_loss_list, 'g-', label='Focal Loss')
        ax.plot(loss_x_list, train_smL1_loss_list, 'm-', label='SmoothL1 Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        current_epoch = loss_x_list[-1] if loss_x_list else 0
        current_loss = train_loss_list[-1] if train_loss_list else 0
        ax.set_title(f'{sub_class} - Training Losses (Epoch {current_epoch}, Loss: {current_loss:.4f})')
        ax.legend()
        ax.grid(True)
        ax.set_yscale('log')
    else:
        ax.text(0.5, 0.5, 'No loss data yet', ha='center', va='center', transform=ax.transAxes)
        ax.set_title('Training Losses (Waiting for data...)')
    
    plt.tight_layout()
    
    if inline:
        # Display inline for Jupyter notebooks
        from IPython.display import display, clear_output
        clear_output(wait=True)
        display(fig)
        plt.close()
        # Print update
        if len(loss_x_list) > 0:
            current_epoch = loss_x_list[-1] if loss_x_list else 0
            current_loss = train_loss_list[-1] if train_loss_list else 0
            print(f"üìä Loss tracking: Epoch {current_epoch}, Loss: {current_loss:.4f}")
    else:
        # Save the plot to file
        os.makedirs(f'{args["output_path"]}/learning_curves/ARGS={args["arg_num"]}', exist_ok=True)
        plt.savefig(f'{args["output_path"]}/learning_curves/ARGS={args["arg_num"]}/{sub_class}_learning_curves.png', 
                    dpi=300, bbox_inches='tight')
        plt.close()
        
        # Print update
        if len(loss_x_list) > 0:
            current_epoch = loss_x_list[-1] if loss_x_list else 0
            current_loss = train_loss_list[-1] if train_loss_list else 0
            print(f"üìä Loss tracking: Epoch {current_epoch}, Loss: {current_loss:.4f}")
            print(f"   üìÅ Saved to: outputs/learning_curves/ARGS={args['arg_num']}/{sub_class}_learning_curves.png")

def is_jupyter_environment():
    """Check if running in Jupyter notebook environment"""
    try:
        from IPython import get_ipython
        return get_ipython() is not None and get_ipython().__class__.__name__ == 'ZMQInteractiveShell'
    except ImportError:
        return False

def signal_handler(signum, frame):
    """Handle Ctrl+C interrupt - save checkpoint before exit"""
    print(f"\nüõë Training interrupted! Saving checkpoint...")
    
    try:
        # Save current training state
        if 'current_models' in globals() and 'current_training_data' in globals():
            unet_model, seg_model = current_models
            args = current_args
            sub_class = current_sub_class
            training_data = current_training_data
            
            interrupt_history = {
                'train_loss_list': training_data['train_loss_list'],
                'train_noise_loss_list': training_data['train_noise_loss_list'],
                'train_focal_loss_list': training_data['train_focal_loss_list'],
                'train_smL1_loss_list': training_data['train_smL1_loss_list'],
                'loss_x_list': training_data['loss_x_list']
            }
            
            current_epoch = training_data['loss_x_list'][-1] if training_data['loss_x_list'] else 0
            save(unet_model, seg_model, args=args, final='last', epoch=current_epoch, 
                 sub_class=sub_class, training_history=interrupt_history)
            
            print(f"üíæ Emergency checkpoint saved at epoch {current_epoch + 1}")
            print(f"üìä You can resume training by setting 'resume_training': true")
        else:
            print("‚ö†Ô∏è  No training data to save")
            
    except Exception as e:
        print(f"‚ùå Error saving checkpoint: {e}")
    
    print("üëã Exiting...")
    sys.exit(0)

def monitor_system_resources():
    """Monitor system resources"""
    # CPU usage
    cpu_percent = psutil.cpu_percent(interval=1)
    
    # Memory usage
    memory = psutil.virtual_memory()
    memory_percent = memory.percent
    memory_available_gb = memory.available / (1024**3)
    
    # GPU memory (if available)
    gpu_memory_info = ""
    if torch.cuda.is_available():
        gpu_memory_allocated = torch.cuda.memory_allocated() / (1024**3)
        gpu_memory_reserved = torch.cuda.memory_reserved() / (1024**3)
        gpu_memory_info = f" | GPU: {gpu_memory_allocated:.2f}GB allocated, {gpu_memory_reserved:.2f}GB reserved"
    
    return f"CPU: {cpu_percent:.1f}% | RAM: {memory_percent:.1f}% ({memory_available_gb:.2f}GB free){gpu_memory_info}"

def train(training_dataset_loader, testing_dataset_loader, args, data_len,sub_class,class_type,device, num_gpus=1):
    
    # Global variables for signal handler
    global current_models, current_args, current_sub_class, current_training_data
    
    # Check if running in Jupyter environment
    use_inline_plots = is_jupyter_environment()
    if use_inline_plots:
        print("üìä Detected Jupyter environment - using inline plots to save memory")
    else:
        print("üìä Using file-based plots")
    
    # Check for resume training
    resume_from_checkpoint = args.get('resume_training', False)
    start_epoch = 0
    
    # Initialize profiler
    profiler = PerformanceProfiler(log_dir=f'{args["output_path"]}/profiling/ARGS={args["arg_num"]}/{sub_class}')
    
    in_channels = args["channels"]
    unet_model = UNetModel(args['img_size'][0], args['base_channels'], channel_mults=args['channel_mults'], dropout=args[
                "dropout"], n_heads=args["num_heads"], n_head_channels=args["num_head_channels"],
            in_channels=in_channels
            ).to(device)


    betas = get_beta_schedule(args['T'], args['beta_schedule'])

    ddpm_sample =  GaussianDiffusionModel(
            args['img_size'], betas, loss_weight=args['loss_weight'],
            loss_type=args['loss-type'], noise=args["noise_fn"], img_channels=in_channels
            )

    seg_model=SegmentationSubNetwork(in_channels=6, out_channels=1).to(device)

    # Enable multi-GPU training if available
    if num_gpus > 1:
        print(f"Wrapping models with DataParallel for {num_gpus} GPUs")
        unet_model = torch.nn.DataParallel(unet_model)
        seg_model = torch.nn.DataParallel(seg_model)

    # Count trainable parameters
    unet_params = count_parameters(unet_model)
    seg_params = count_parameters(seg_model)
    total_params = unet_params + seg_params
    
    print(f"Model Parameters:")
    print(f"  - UNet Model: {unet_params:,} parameters")
    print(f"  - Segmentation Model: {seg_params:,} parameters") 
    print(f"  - Total Trainable Parameters: {total_params:,} parameters")
    print(f"  - Memory (approx): {total_params * 4 / 1024**2:.1f} MB (FP32)")

    optimizer_ddpm = optim.Adam( unet_model.parameters(), lr=args['diffusion_lr'],weight_decay=args['weight_decay'])
    
    optimizer_seg = optim.Adam(seg_model.parameters(),lr=args['seg_lr'],weight_decay=args['weight_decay'])
    
    # Initialize mixed precision scaler
    use_mixed_precision = args.get('use_mixed_precision', True) and torch.cuda.is_available()
    scaler = GradScaler() if use_mixed_precision else None
    
    # Gradient accumulation settings
    gradient_accumulation_steps = args.get('gradient_accumulation_steps', 1)
    effective_batch_size = args['Batch_Size'] * gradient_accumulation_steps
    
    print(f"Mixed Precision (FP16): {'Enabled' if use_mixed_precision else 'Disabled'}")
    print(f"Gradient Accumulation Steps: {gradient_accumulation_steps}")
    print(f"Effective Batch Size: {effective_batch_size}")

    loss_focal = BinaryFocalLoss().to(device)
    loss_smL1= nn.SmoothL1Loss().to(device)
    

    scheduler_seg =optim.lr_scheduler.CosineAnnealingLR(optimizer_seg, T_max=10, eta_min=0, last_epoch=- 1)
    
    # Initialize training variables
    train_loss_list=[]
    train_noise_loss_list=[]
    train_focal_loss_list=[]
    train_smL1_loss_list=[]
    loss_x_list=[]
    best_loss=float('inf')  # Track best (lowest) loss
    best_epoch=0
    image_auroc_list=[]
    pixel_auroc_list=[]
    performance_x_list=[]
    
    # Resume training if checkpoint exists
    if resume_from_checkpoint:
        checkpoint_path = f'{args["output_path"]}/model/diff-params-ARGS={args["arg_num"]}/{sub_class}/params-last.pt'
        if os.path.exists(checkpoint_path):
            print(f"üîÑ Resuming training from {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # Load model states
            if hasattr(unet_model, 'module'):
                unet_model.module.load_state_dict(checkpoint['unet_model_state_dict'])
                seg_model.module.load_state_dict(checkpoint['seg_model_state_dict'])
            else:
                unet_model.load_state_dict(checkpoint['unet_model_state_dict'])
                seg_model.load_state_dict(checkpoint['seg_model_state_dict'])
            
            start_epoch = checkpoint['n_epoch']
            print(f"üéØ Resuming from epoch {start_epoch}")
            
            # Load loss history if available
            if 'train_loss_list' in checkpoint:
                train_loss_list = checkpoint.get('train_loss_list', [])
                train_noise_loss_list = checkpoint.get('train_noise_loss_list', [])
                train_focal_loss_list = checkpoint.get('train_focal_loss_list', [])
                train_smL1_loss_list = checkpoint.get('train_smL1_loss_list', [])
                loss_x_list = checkpoint.get('loss_x_list', [])
                best_loss = checkpoint.get('best_loss', float('inf'))
                best_epoch = checkpoint.get('best_epoch', 0)
                print(f"üìä Loaded loss history: {len(train_loss_list)} epochs")
                print(f"üèÜ Best loss so far: {best_loss:.4f} at epoch {best_epoch + 1}")
        else:
            print(f"‚ö†Ô∏è  No checkpoint found at {checkpoint_path}, starting from scratch")
            start_epoch = 0
    
    # Setup Ctrl+C handler
    signal.signal(signal.SIGINT, signal_handler)
    
    # Update global variables for signal handler
    current_models = (unet_model, seg_model)
    current_args = args
    current_sub_class = sub_class
    current_training_data = {
        'train_loss_list': train_loss_list,
        'train_noise_loss_list': train_noise_loss_list,
        'train_focal_loss_list': train_focal_loss_list,
        'train_smL1_loss_list': train_smL1_loss_list,
        'loss_x_list': loss_x_list
    }
    
    tqdm_epoch = range(start_epoch, args['EPOCHS'])
    
    # Performance monitoring
    epoch_times = []
    resource_monitor_interval = 100  # Monitor resources every 100 epochs
    
    for epoch in tqdm_epoch:
        epoch_start_time = time.time()
        unet_model.train()
        seg_model.train()
        train_loss = 0.0
        train_focal_loss=0.0
        train_smL1_loss = 0.0
        train_noise_loss = 0.0
        tbar = tqdm(training_dataset_loader)
        
        # Initialize gradient accumulation
        optimizer_ddpm.zero_grad()
        optimizer_seg.zero_grad()
        
        for i, sample in enumerate(tbar):
            batch_start_time = time.time()
            
            # Profile data loading
            with profiler.timer('data_loading'):
                aug_image=sample['augmented_image'].to(device)
                anomaly_mask = sample["anomaly_mask"].to(device)
                anomaly_label = sample["has_anomaly"].to(device).squeeze()

            # Profile forward pass
            with profiler.timer('forward_pass'):
                # Mixed precision forward pass
                if use_mixed_precision:
                    with autocast('cuda'):
                        noise_loss, pred_x0,normal_t,x_normal_t,x_noiser_t = ddpm_sample.norm_guided_one_step_denoising(unet_model, aug_image, anomaly_label,args)
                        pred_mask = seg_model(torch.cat((aug_image, pred_x0), dim=1)) 

                        #loss
                        focal_loss = loss_focal(pred_mask,anomaly_mask)
                        smL1_loss = loss_smL1(pred_mask, anomaly_mask)
                        loss = (noise_loss + 5*focal_loss + smL1_loss) / gradient_accumulation_steps
                else:
                    noise_loss, pred_x0,normal_t,x_normal_t,x_noiser_t = ddpm_sample.norm_guided_one_step_denoising(unet_model, aug_image, anomaly_label,args)
                    pred_mask = seg_model(torch.cat((aug_image, pred_x0), dim=1)) 

                    #loss
                    focal_loss = loss_focal(pred_mask,anomaly_mask)
                    smL1_loss = loss_smL1(pred_mask, anomaly_mask)
                    loss = (noise_loss + 5*focal_loss + smL1_loss) / gradient_accumulation_steps
            
            # Profile backward pass
            with profiler.timer('backward_pass'):
                # Mixed precision backward pass
                if use_mixed_precision:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

            # Profile optimizer step
            with profiler.timer('optimizer_step'):
                # Gradient accumulation step
                if (i + 1) % gradient_accumulation_steps == 0:
                    if use_mixed_precision:
                        scaler.step(optimizer_ddpm)
                        scaler.step(optimizer_seg)
                        scaler.update()
                    else:
                        optimizer_ddpm.step()
                        optimizer_seg.step()
                    
                    scheduler_seg.step()
                    optimizer_ddpm.zero_grad()
                    optimizer_seg.zero_grad()

            train_loss += loss.item() * gradient_accumulation_steps  # Scale back for logging
            tbar.set_description('Epoch:%d, Train loss: %.3f' % (epoch, train_loss))

            train_smL1_loss += smL1_loss.item()
            train_focal_loss+=5*focal_loss.item()
            train_noise_loss+=noise_loss.item()
            
            # Log batch metrics
            batch_time = time.time() - batch_start_time
            profiler.log_training_metrics(
                epoch=epoch,
                batch_idx=i,
                losses={
                    'total_loss': loss.item() * gradient_accumulation_steps,
                    'noise_loss': noise_loss.item(),
                    'focal_loss': focal_loss.item(),
                    'smL1_loss': smL1_loss.item()
                },
                batch_time=batch_time
            )
            
        # Record epoch time
        epoch_time = time.time() - epoch_start_time
        epoch_times.append(epoch_time)
        
        # Log system metrics and monitor resources
        profiler.log_system_metrics()
        profiler.log_training_metrics(epoch=epoch, batch_idx=0, losses={}, epoch_time=epoch_time)
        
        # Monitor system resources periodically
        if epoch % resource_monitor_interval == 0 and epoch > 0:
            resource_info = monitor_system_resources()
            avg_epoch_time = np.mean(epoch_times[-resource_monitor_interval:]) if len(epoch_times) >= resource_monitor_interval else np.mean(epoch_times)
            print(f"Epoch {epoch} | {resource_info} | Avg Epoch Time: {avg_epoch_time:.2f}s")
            
            # Check for bottlenecks
            bottlenecks = profiler.detect_bottlenecks()
            if bottlenecks:
                print("‚ö†Ô∏è  Performance bottlenecks detected:")
                for bottleneck in bottlenecks:
                    print(f"   - {bottleneck}")
            
            # Clear GPU cache periodically
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

        # Save losses every epoch (always track progress)
        train_loss_list.append(round(train_loss,3))
        train_smL1_loss_list.append(round(train_smL1_loss,3))
        train_focal_loss_list.append(round(train_focal_loss,3))
        train_noise_loss_list.append(round(train_noise_loss,3))
        loss_x_list.append(int(epoch))
        
        # Update global variables for signal handler
        current_training_data.update({
            'train_loss_list': train_loss_list,
            'train_noise_loss_list': train_noise_loss_list,
            'train_focal_loss_list': train_focal_loss_list,
            'train_smL1_loss_list': train_smL1_loss_list,
            'loss_x_list': loss_x_list
        })
        
        # Plot learning curves every epoch for first 10 epochs, then every 10 epochs
        if epoch < 10 or epoch % 10 == 0:
            plot_learning_curves(train_loss_list, train_noise_loss_list, train_focal_loss_list, 
                                train_smL1_loss_list, loss_x_list, image_auroc_list, 
                                pixel_auroc_list, performance_x_list, sub_class, args, inline=use_inline_plots)

        # Check if this is the best loss so far
        if train_loss < best_loss:
            best_loss = train_loss
            best_epoch = epoch
            
            # Save best model
            best_history = {
                'train_loss_list': train_loss_list,
                'train_noise_loss_list': train_noise_loss_list,
                'train_focal_loss_list': train_focal_loss_list,
                'train_smL1_loss_list': train_smL1_loss_list,
                'loss_x_list': loss_x_list,
                'best_loss': best_loss,
                'best_epoch': best_epoch
            }
            save(unet_model, seg_model, args=args, final='best', epoch=epoch, sub_class=sub_class, training_history=best_history)
            print(f"üèÜ New best loss: {best_loss:.4f} at epoch {epoch + 1} - Best model saved!")

        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            checkpoint_history = {
                'train_loss_list': train_loss_list,
                'train_noise_loss_list': train_noise_loss_list,
                'train_focal_loss_list': train_focal_loss_list,
                'train_smL1_loss_list': train_smL1_loss_list,
                'loss_x_list': loss_x_list,
                'best_loss': best_loss,
                'best_epoch': best_epoch
            }
            save(unet_model, seg_model, args=args, final='last', epoch=epoch, sub_class=sub_class, training_history=checkpoint_history)
            print(f"üíæ Checkpoint saved at epoch {epoch + 1}")

        # Skip evaluation during training - focus only on loss
                
            
    # Save final checkpoint with training history (loss only)
    final_training_history = {
        'train_loss_list': train_loss_list,
        'train_noise_loss_list': train_noise_loss_list,
        'train_focal_loss_list': train_focal_loss_list,
        'train_smL1_loss_list': train_smL1_loss_list,
        'loss_x_list': loss_x_list,
        'best_loss': best_loss,
        'best_epoch': best_epoch
    }
    save(unet_model,seg_model, args=args,final='last',epoch=args['EPOCHS']-1,sub_class=sub_class, training_history=final_training_history)

    # Skip final plot to save memory
    
    # Save training statistics (loss-focused)
    final_loss = train_loss_list[-1] if train_loss_list else 0
    training_stats = {
        'total_epochs': len(train_loss_list),
        'final_loss': final_loss,
        'best_loss': best_loss,
        'best_epoch': best_epoch + 1,  # Human-readable (1-indexed)
        'avg_epoch_time': np.mean(epoch_times) if epoch_times else 0,
        'total_training_time': sum(epoch_times) if epoch_times else 0
    }
    
    with open(f'{args["output_path"]}/metrics/ARGS={args["arg_num"]}/{sub_class}_training_stats.json', 'w') as f:
        json.dump(training_stats, f, indent=2)

    # Save profiler results (text only, no plots)
    profiler.print_summary()
    profiler.save_stats()
   


def save(unet_model,seg_model, args,final,epoch,sub_class, training_history=None):
    
    # Handle DataParallel models - save the underlying module
    unet_state_dict = unet_model.module.state_dict() if hasattr(unet_model, 'module') else unet_model.state_dict()
    seg_state_dict = seg_model.module.state_dict() if hasattr(seg_model, 'module') else seg_model.state_dict()
    
    # Convert defaultdict to regular dict for PyTorch 2.6+ compatibility
    args_dict = dict(args) if hasattr(args, 'keys') else args
    
    # Base checkpoint data
    checkpoint_data = {
        'n_epoch':              epoch,
        'unet_model_state_dict': unet_state_dict,
        'seg_model_state_dict':  seg_state_dict,
        "args":                 args_dict
    }
    
    # Add training history for resume functionality
    if training_history is not None:
        checkpoint_data.update(training_history)
    
    torch.save(checkpoint_data, f'{args["output_path"]}/model/diff-params-ARGS={args["arg_num"]}/{sub_class}/params-{final}.pt')
    
    

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device = torch.device("cpu")
    
    # Check for multiple GPUs
    num_gpus = torch.cuda.device_count()
    print(f"Number of available GPUs: {num_gpus}")
    if num_gpus > 1:
        print(f"Using {num_gpus} GPUs for training")
    elif num_gpus == 1:
        print("Using single GPU for training")
    else:
        print("Using CPU for training")
    
    # read file from argument
    file = "args1.json"
    # load the json args
    with open(f'./args/{file}', 'r') as f:
        args = json.load(f)
    args['arg_num'] = file[4:-5]
    args = defaultdict_from_json(args)

    real_iad_classes = os.listdir(os.path.join(args["data_root_path"], args['data_name']))

    for sub_class in real_iad_classes:   
        print("class", sub_class)
        
        subclass_path = os.path.join(args["data_root_path"], args['data_name'], sub_class)
        
        training_dataset = RealIADTrainDataset(
            subclass_path, sub_class, img_size=args["img_size"], args=args
        )
        testing_dataset = RealIADTestDataset(
            subclass_path, sub_class, img_size=args["img_size"]
        )
        class_type=args['data_name']
        

        print(file, args)     

        data_len = len(testing_dataset)
        
        # Check if test dataset is empty
        if data_len == 0:
            print(f"‚ö†Ô∏è  WARNING: Test dataset for {sub_class} is empty!")
            print(f"   - Training will continue but no evaluation will be performed")
            print(f"   - Check data path: {args['data_root_path']}/{args['data_name']}/{sub_class}")
        
        # Calculate effective batch size considering multi-GPU and gradient accumulation
        base_batch_size = args['Batch_Size']
        gradient_accumulation_steps = args.get('gradient_accumulation_steps', 1)
        
        # For DataLoader, we use the base batch size
        dataloader_batch_size = base_batch_size
            
        # Total effective batch size
        total_effective_batch_size = dataloader_batch_size * gradient_accumulation_steps
        
        print(f"Batch size configuration:")
        print(f"  - Base batch size: {base_batch_size}")
        print(f"  - DataLoader batch size: {dataloader_batch_size} ({'Multi-GPU' if num_gpus > 1 else 'Single-GPU'})")
        print(f"  - Gradient accumulation steps: {gradient_accumulation_steps}")
        print(f"  - Total effective batch size: {total_effective_batch_size}")

        
        # Optimize DataLoader settings
        optimal_num_workers = min(8, os.cpu_count() // 2) if os.cpu_count() else 4
        print(f"Using {optimal_num_workers} workers for data loading")
        
        training_dataset_loader = DataLoader(
            training_dataset, 
            batch_size=dataloader_batch_size,
            shuffle=True,
            num_workers=optimal_num_workers,
            pin_memory=True,
            drop_last=True,
            persistent_workers=True,  # Keep workers alive between epochs
            prefetch_factor=2  # Prefetch 2 batches per worker
        )
        test_loader = DataLoader(
            testing_dataset, 
            batch_size=1,
            shuffle=False, 
            num_workers=min(4, optimal_num_workers),
            pin_memory=True,
            persistent_workers=True
        )

        # make arg specific directories
        for i in [f'{args["output_path"]}/model/diff-params-ARGS={args["arg_num"]}/{sub_class}',
                f'{args["output_path"]}/diffusion-training-images/ARGS={args["arg_num"]}/{sub_class}',
                 f'{args["output_path"]}/metrics/ARGS={args["arg_num"]}/{sub_class}']:
            try:
                os.makedirs(i)
            except OSError:
                pass

        train(training_dataset_loader, test_loader, args, data_len,sub_class,class_type,device, num_gpus)

if __name__ == '__main__':
    
    seed(42)
    main()